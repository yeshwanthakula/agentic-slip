apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: llm-nodepool
spec:
  # Disruption settings for cost optimization
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
    expireAfter: 30m
  
  # Template for nodes
  template:
    metadata:
      labels:
        workload-type: "llm-inference"
    spec:
      # Node requirements
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"] # Prefer spot for cost savings
        - key: node.kubernetes.io/instance-type
          operator: In
          values: 
            - "g4dn.xlarge"    # GPU instances for LLM
            - "g4dn.2xlarge"
            - "g5.xlarge"
            - "g5.2xlarge"
            - "m5.large"       # CPU instances for other services
            - "m5.xlarge"
            - "c5.large"
            - "c5.xlarge"
      
      # Node configuration
      nodeClassRef:
        name: llm-nodeclass
      
      # Taints for GPU nodes (LLM workloads only)
      taints:
        - key: nvidia.com/gpu
          value: "true"
          effect: NoSchedule

---
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: llm-nodeclass
spec:
  # AMI selection
  amiFamily: AL2
  
  # Subnet and security group configuration
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: "llm-inference-cluster"
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: "llm-inference-cluster"
  
  # Instance profile
  instanceProfile: "KarpenterNodeInstanceProfile-llm-inference-cluster"
  
  # User data for GPU setup
  userData: |
    #!/bin/bash
    /etc/eks/bootstrap.sh llm-inference-cluster
    
    # Install NVIDIA drivers for