apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-qwen
  labels:
    app: vllm
    model: qwen-0.5b
spec:
  replicas: 0  # Start with 0 for KEDA scaling
  selector:
    matchLabels:
      app: vllm
      model: qwen-0.5b
  template:
    metadata:
      labels:
        app: vllm
        model: qwen-0.5b
    spec:
      # FIXED: tolerations should be at spec level, not mixed with volumes
      tolerations:
      - key: "workload"
        value: "vllm"
        effect: "NoSchedule"
      
      containers:
        - name: vllm-server
          image: vllm/vllm-openai:v0.3.3
          ports:
            - containerPort: 8000
              name: http
          
          # REDUCED resources to fit your nodes
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "500m"
          
          # CORRECTED: No command, just args
          args:
            - --model=Qwen/Qwen1.5-0.5B-Chat
            - --host=0.0.0.0
            - --port=8000
            - --served-model-name=qwen-0.5b
            - --trust-remote-code
            - --max-model-len=1024
            - --download-dir=/model-cache
            - --disable-log-requests
            - --tensor-parallel-size=1
            - --device=cpu
            - --dtype=float16
            - --load-format=auto
            - --max-num-seqs=4
          
          volumeMounts:
            - name: model-cache
              mountPath: /model-cache
          
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 900
            periodSeconds: 30
          
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8000
            initialDelaySeconds: 600
            periodSeconds: 15
      
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-model-storage